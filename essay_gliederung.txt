Gliederung:
1. Heranführen an das Problem mit einer kurzen, auf das Thema zugeschnittenen Trolley-Problem-Variante (besserer Name? Vielleicht aus einer der Quellen?)
	Fred (oder IRA: intelligent road agent) fährt in einer regnerischen Herbstnacht durch die Stadt. Es sind keine Menschen mehr unterwegs, die Straßen sind frei und Fred kann die Grüne Welle (Fußnote) optimal nutzen. 
	Direkt nach einer Kurve sieht Fred auf der Straße zwei Frauen und bremst. Durch das nasse Laub kann er nicht mehr rechtzeitig zum stehen kommen und kann nur noch, 
	von Gebäuden umgeben, die Richtung und somit eine der beiden geschockten Frauen als Ziel bestimmen. Die Entscheidung fällt auf die Fülligere, da sie mit großer Wahrscheinlichkeit
	eine geringere Lebenserwartung hat als die andere, etwa gleichaltige Frau hat. Die füllige Frau stirbt an Ort und Stelle. Fred hat kein schlechtes Gewissen. Er hat seine Insassen
	geschützt und seinen Algorithmen entsprechend gehandelt. Leider war die verstorbene Frau nicht einfach übergewichtig, sondern hochschwanger. Die Angehörigen werden den 	
	Automobilhersteller für ihren Verlust haftbar machen. 

	Probleme hier: Uhrzeit, Witterung (auch geringe Geschwindigkeit schon gefährlich, aber man muss ja auch voran kommen), Umgebung (keine Ausweichmöglichkeit), 
	Geschwindigkeit (an sich effizient), unbewegliche "Opfer", Targeting System (Zitat) hat falsch entschieden, Insassen um jeden Preis schützen, Haftung. Fred pflegt einen effizienten, den
	StVO entsprechenden Fahrstil und tötet einen Menschen. aber erstmal Grundlegendes klären, um eine objektivere Bewertung der Umstände zu ermöglichen.
2. Einführende Begriffe und Sachverhalte und Annahmen:
	Automatisierungsgrade
	dynamische Elemente
	Ich gehe im Folgenden davon aus, dass AA perfekt funktioniert, also ideale Bedingungen herrschen bei Betrachten der Dilemma (keine Bugs, fehlerhaften Sensoren, Hacker...)
	relevante Ethiken (und ein Beispiel: Tugendethik, warum man hier drauf verzichten kann - zu menschlich)
	Ethik als Teil der Philosophie
	StVO und FEV (Grundlegendes zur Anforderung an den menschlichen Autofahrer - sollte ein Computer zur MPU?)
3. Die Dilemma bzw grundlgende Probleme
	Als Use-Case beschreiben bzw. Trolley-Problem schildern (präferiert: Motorradfahrer) vielleicht mit Pseudocode von IRA?
	Mensch ist nicht für Monotonie geschaffen (Autobahn), AA soll entlasten, trotzdem soll Mensch überwachen obwohl noch weniger passiert? Kritik an Automatisierungsgrade bzw Haftung des Fahrers 
	(dann sind AA eigentlich Quatsch; es sollte schon drauf hinauslaufen, dass die Insassen sich mit anderen Dingen beschäftigen DÜRFEN)
3.a Warum ist das überhaupt wichtig? Viel zu theoretisch, selten...? KURZ HALTEN!! ABSCHNITT 4 IST HAUPTTEIL!!
	Erhoffte Ziele des AA erläutern (weniger Tote, umweltfreundlich, car sharing, bessere Nutzung der Straßen in Großstädten, soziale Änderungen...)
	Wer wäre haftbar? Bezug auf Dilemma: Mensch soll trotz AA immer genau aufpassen und jederzeit eingreifen können
	Menschen sind in Schocksituationen unzulänglich, genau dafür würde AA eingeführt, aber... man wollte es zu optimal lösen und stößt auf Grenzen der Moral?
	Wer zahlt für Unfälle
4. Mögliche ethische Sichten/Auslegungen bzw wie wird das Problem der Dilemma-Situation analysiert
	welche Ethik wird vertreten? warum wird eine andere abgelehnt? wird sie konsequent vertreten? Kritik am Autor?
	wie wird sie ausgelegt; wird sie anders ausgelegt als bei Person XY?
	Jeder sieht es anders (auch die Gesellschaft), wie soll man da konkret etwas in Code implementieren?
	Menschen werden zwar gerettet, aber es sterben welche, die ohne AA nicht gestorben wären durch Verkehrsunfall (Gegenargument: Autoglas bzw Fortschritt im Allgemeinen; Folgen sind nicht unbedingt 
	absehbar)
	Problem: Quantifizierung des nicht-messbaren; aber Computer können nur mit Zahlen umgehen. 
5. Lösungsansätze
	Je nach U.-Ethik: schon das Abwägen von menschlichen Leben verboten, also in solchen Fällen keine Entscheidung treffen (auch wenn dann die meisten Menschen sterben)
	Strategiepapier der Bundesregierung
	Code offenlegen
	Randomisieren, also in etwa den perfekten menschlichen Fahrer simulieren (verstößt gegen erhofftes Ziel: Menschen retten, da sie ja rettbar gewesen werden)
	Regierung regeln lassen
	Autobauer regeln lassen (immer Insassen schützen)
	(Mandatory vs Personal - "Im Dilemma bitte immer Christen töten, danke.")
	Utilitarismus als Grund, warum AA nicht eingeführt werden könnte - widerspricht dem Utilitarismus (größten Nutzen erzielen, hier also Menschenleben retten)
	Extrem passives Fahren (undenkbar)
	wird man solche Dilemma-Todesfälle schlicht als Unglück hinnehmen? Man kann einen menschlichen Fahrer auch nicht für eine (wohlmöglich falsche) Entscheidung in Bruchteilen von Sekunden verantwortlich
	machen
6. Schluss
	Überschreitung der Landesgrenze = anderes System? (Indien: Kuh wertvoller als Mensch; sry für den Stereotyp)
	
	
