\documentclass[a4paper, 12pt, titlepage]{scrartcl} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{babel}
\usepackage[onehalfspacing]{setspace} %sollte 1,5 Zeilenabstand erzeugen
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,includeheadfoot]{geometry}

\usepackage[babel,german=guillemets]{csquotes}
\usepackage[style=ieee]{biblatex} %[style=alphabetic oder numeric etc.,backend=biber]
\bibliography{essay_document_biblio}

\begin{document}
\author{Anne Borchard}
\title{Ethische Anforderungen an autonom fahrende Personenkraftwagen im Stra\ss enverkehr}
\publishers{Humboldt-Universit\"at zu Berlin}
\maketitle
\tableofcontents
\newpage

\section{Einleitung}
	IRA\footnote{Intelligent Road Agent; fiktives, voll-autonomes Assistenzsystem} fährt in einer regnerischen Nach durch die Stadt. Die Straßen sind leer und sie kann die Ampelschaltung optimal nutzen. Direkt nach einer Kurve sieht IRA auf der Straße zwei Frauen und bremst. Nasses Laub verl\"angert den Bremsweg drastisch, IRA hat nur folgende Optionen: bei Beibehaltung des Kurses beide Frauen treffen oder sich f\"ur eine Frau als Ziel entscheiden. Die Entscheidung fällt auf die \"Ubergewichtige (Bezeichnung: A), da sie mit großer Wahrscheinlichkeit eine geringere Lebenserwartung hat als die Frau. A stirbt an Ort und Stelle. IRA hat ihre Insassen geschützt und ihren Algorithmen entsprechend gehandelt. Leider war A nicht übergewichtig, sondern hochschwanger. Die Angehörigen von A werden versuchen, den Automobilhersteller für ihren Verlust haftbar zu machen.
	
	Dieses Dilemma kann sich, wenn auch mit geringer Wahrscheinlichkeit, in einer Zukunft mit autonom fahrenden Fahrzeugen zutragen. Es zeigt, dass sich die Gesellschaft mit den moralischen Dimensionen des autonomen Fahrens auseinandersetzen muss. Welche ethischen Richtlinien sollen solche Fahrzeuge befolgen? Wer entscheidet dar\"uber? Wie k\"onnen Dilemma dieser Art gel\"ost werden? Besteht \"uberhaupt Konsens \"uber die \glqq richtige\grqq{} Ethik?
\section{Begriffe und Annahmen}
	Im folgendem Essay werden die Begriffe autonomer Agent und autonom fahrendes Fahrzeug synonym verwendet. Da es in der Literatur verschiedenen Graden der Automatisierung gibt, wird, falls nicht anders erw\"ahnt, vom h\"ochsten Grad\footnote{Laut Bundesministerium für Verkehr und digitale Infrastruktur: Fahrerloses Fahren, Insassen sind reine Passagiere (vgl. \autocite{bmvi2015}, S. 6).} ausgegangen. Zudem wird angenommen, dass der autonome Agent im technischen Sinne perfekt funktioniert\footnote{Implementierungsfehler, fehlerhafte Sensoren oder unbefugte Eingriffe von Au\ss en werden ausgeschlo\ss en, sodass der autonome Agent in einem idealen Umfeld agieren kann.}.
	
	Da sich das Essay an Personen aus dem Bereich der Informatik richtet, werden im Folgenden zwei hierf\"ur relevante Formen der Ethik kurz erl\"autert.
	Auf der einen Seite wird die \emph{Deontologische Ethik}, auch als \emph{Deontologie} bezeichnet, thematisiert. Dabei handelt es sich um eine moralische Handlungsanweisung, welche eine strikte Befolgung von Regeln oder auch Pflichten verlangt (vgl. \autocite{baase:fire}, S. 47). Die Konsequenzen einer Handlung, egal ob im Allgemeinen als w\"unschenswert angesehen oder nicht, werden nicht betrachtet. 
	
	Auf der anderen Seite ist die \emph{Utilitaristischen Ethik} als Auspr\"agung der \emph{Konsequenzialistische Ethik} wichtiger Bestandteil folgender Analysen. Hierbei werden moralische Entscheidungen ausschlie\ss lich anhand ihrer direkten und indirekten Folgen getroffen. Baase \autocite{baase:fire} formuliert wie folgt: \glqq we consider the impact on utility and judge the action by its net impact\grqq{} (\autocite{baase:fire}, S. 48).
\section{Grundlegende Probleme}
	Um in Deutschland ein Fahrzeug im \"offentlichen Stra\ss enverkehr bewegen zu d\"urfen, ist, sofern nicht anders vermerkt, eine Fahrerlaubnis n\"otig. Das Regelwerk hierzu bildet die FeV (Fahrerlaubnis-Verordnung). Ziel ist es, sowohl das Fahrzeug kontrollieren als auch im Rahmen der StVO (Stra\ss enverkehrs-Ordnung) am allgemeinen Stra\ss enverkehr teilnehmen zu k\"onnen. Im Gegensatz zu einem menschlichen Fahrer werden an einen autonomen Agenten offensichtlich Anforderungen gestellt, welche \"uber technische Aspekte hinaus gehen. N\"urnberger formuliert es treffend: \glqq Interessanter Weise haben wir an Algorithmen höhere ethische Ansprüche als an uns selbst\grqq{} (\cite{nunu2016}, S. 2).

	Zu den angestrebten Zielen durch die Verwendung autonomer Agenten geh\"oren die deutliche Reduzierung von Verkehrstoten, die effizientere Nutzung der Infrastruktur (mit einhergehender Verringerung des Kraftstoffverbrauchs) und das Integrieren von Personenkreisen, welche zuvor nicht am Stra\ss enverkehr ungehindert teilnehmen konnten. Die Einf\"uhrung von autonomen Agenten ist unter Betrachtung dieser Aspekte f\"ur weite Teile der Bev\"olkerung w\"unschenswert.
	
	Den erhofften Vorz\"uge stehen jedoch Probleme gegen\"uber. Ein autonomer Agent kann in einer wie in der Einleitung geschilderten Situation \emph{Entscheidungen treffen}, w\"ahrend ein menschlicher Fahrer in Bruchteilen von Sekunden fast ausschlie\ss lich \emph{reagieren} kann. Wie jedoch kommt der Agent zu seinen Entscheidungen? Der Programmierer\footnote{Aus Gr\"unden der Lesbarkeit wird in vielen F\"allen die maskuline Form eines Wortes gew\"ahlt, jedoch sind beide Geschlechter angesprochen.} des Entscheidungsalgorithmus hat genug Zeit im Voraus, um situationsabh\"angig einen Ablaufplan vorzubereiten. Er hat programmiert, ob und welche Abw\"agungen in Dilemma-Situationen getroffen werden. Im Folgenden werden verschiedene ethische Sichtweisen auf die geschilderten Probleme und teilweise auch L\"osungsvorschl\"age erl\"autert.   
\section{Deontologische Sichtweise}
	Eine rein deontologische Ethik scheint als Algorithmus perfekt implementierbar zu sein, jedoch bietet sie keine L\"osung f\"ur Dilemma-Situationen. Regeln wie die StVO dienen als Rahmen f\"ur \emph{allt\"agliche} Situationen im Stra\ss enverkehr, m\"ussen aber im Interessenkonflikt gegebenenfalls gebrochen werden. Man stelle sich vor: Ein Krankenwagen erreicht verz\"ogert einem Unfallort, weil ein menschlicher Autofahrer der StVO gem\"a\ss\ nicht die rote Ampel \"uberfahren hat, um dem Krankenwagen Platz zu machen. Aus diesem Grund w\"urden f\"ur autonome Agenten \glqq soft constraint\grqq{} (\autocite{Gerdes2015}, S. 94) gebraucht, die den Umst\"anden entsprechend gebrochen werden d\"urfen. Diese Forderung widerspricht der reinen deontologischen Ethik. 
\section{Utilitaristische Sichtweise}
	Wie die deontologische hat auch die utilitaristische Ethik den Vorzug, vergleichsweise einfach in eine algorithmische Form gebracht werden zu k\"onnen. Elemente des Stra\ss enverkehrs werden als Variablen mit festgelegten Werten belegt, im Entscheidungsfall miteinander abgewogen und m\"ussen gewissen Konstanten entsprechen (z.B. StVO). Computer sind f\"ur den Umgang mit Zahlen pr\"adestiniert. Es gestaltet sich schwierig, sowohl den richtigen Wert jener Elemente zu ermitteln, als auch nicht-quantifizierbare Attribute -- z.B. der Wert eines Menschen -- f\"ur einen Computer messbar zu machen. 

	Reschka \autocite{Reschka2015} spricht sich implizit f\"ur den utilitaristischen Ansatz aus, da durch kontinuierlicher Ermittlung und Einhaltung eines sicheren Zustandes im Falle eines Unfalls der entstehende Schaden m\"oglichst minimal bleiben solle (vgl. \autocite{Reschka2015}, S. 491). Zudem schl\"agt er vor, das Dilemma technisch zu begegnen: Autonome Agenten sollten in der Lage sein, durch Kommunikation untereinander sich vorausschauend f\"ur die Handlungsoption mit dem geringsten erwarteten Schaden zu entscheiden (vgl. \autocite{Reschka2015}, S. 507).
	
	Gogoll und M\"uller \autocite{Gogoll2016} hingegen stellten sich zun\"achst die Frage: Sollte die Ethik eines autonomen Agenten durch die Gesellschaft oder durch den Einzelnen selbst festgelegt werden (vgl. \autocite{Gogoll2016}, S. 3)? Ihren Berechnungen zufolge best\"unde das Optimum -- das Wohl der meisten Menschen -- darin, ein utilitaristisches, staatlich vorgegebenes Modell zu implementieren, was im Extremfall die Opferung der eigenen Insassen des autonomen Agenten einschl\"o\ss e (vgl. \autocite{Gogoll2016}, S. 12 ff.). Jedoch r\"umen sie ein, dass viele Menschen sich gegen einen selbst opfernden autonomen Agenten entscheiden w\"urden, obwohl dieser im Sinne aller w\"are (vgl. \autocite{Gogoll2016}, S. 18). 
	
	In der Annahme, dass sich Unf\"alle mit autonomen Agenten nicht vermeiden lassen werden, spricht Lin \autocite{Lin2015} gar von einer \glqq crash-optimization\grqq{} (\autocite{Lin2015}, S. 72), welche f\"ur autonome Agenten in Betrachtung gezogen werden k\"onnte. Lin \autocite{Lin2015} f\"uhrt an, der Blickwinkel entscheide hierbei, \emph{wessen} Schaden minimiert werden soll. Wenn als \glqq Ziele\grqq{} eine Grundsch\"ulerin, ihre 80-j\"ahrige Gro\ss mutter oder ein massiver SUV\footnote{Sport Utility Vehicle} zur Auswahl st\"unden, g\"abe es unterschiedliche Arten, den minimalen Schaden zu berechnen. F\"ur die Insassen des autonomen Agenten w\"are das Kind das leichteste Objekt und somit bei einer Kollision eine geringere Gefahr, verletzt oder get\"otet zu werden. Ist beispielsweise nur ein Insasse im autonomen Agenten, k\"onnte es optimal sein, in den SUV zu steuern, da dessen Insasse(n) mit gro\ss er Wahrscheinlichkeit unverletzt blieben. Somit w\"are nur eine Person, der Insasse des autonomen Agenten, zu Schaden gekommen. Die \"altere Dame k\"ame unter dem Aspekt infrage, als dass sie bereits ein langes Leben gef\"uhrt hat und es ein Unrecht w\"are, das Leben von deutlich j\"ungeren Personen zu beenden (vgl. \autocite{Lin2015}, S. 70 ff.). Problematisch wird es, falls bekannt ist, \emph{wie} autonome Agenten Unf\"alle zu vermeiden versuchen und \glqq other drivers may be tempted to 'game' it\grqq{} (\autocite{Lin2015}, S. 81). 
	
	Es ist unwahrscheinlich, dass sich in einer heterogenen, demokratischen Gesellschaft eine Mehrheit f\"ur ein Modell finden lie\ss e, ohne massive Einw\"ande von anderer Teilen der Bev\"olkerung zu provozieren. Die zuvor genannten Modelle sollte kritisch hinterfragt werden: Auf der einen Seite w\"urden bestimmte gesellschaftliche Gruppen (z.B. Senioren aufgrund ihres Alters) \"ofter als \glqq Ziel\grqq{} von autonomen Agenten gew\"ahlt werden als andere. Auf der anderen Seite sollte es die Aufgabe jedes demokratischen Staates sein, das individuelle Leben zu sch\"utzen und nicht f\"ur das Wohl der Mehrheit zu opfern (vgl. \autocite{Hevelke2015}, S. 622). Es k\"onnte passieren, dass ein utilitaristischer, im Notfall sich selbst opfernder autonomer Agent seine Einf\"uhrung in den allgemeinen Stra\ss enverkehr verz\"ogert oder gar verhindert, womit die erhofften Ziele des autonomen Fahrens nicht mehr erreicht w\"urden. Der Utilitarismus st\"unde sich paradoxerweise selbst im Wege (vgl. \autocite{Bonnefon1573}, S. 1575 f.).
\section{Hybride und andere Sichtweisen}
	Aus dem vorhergehenden Kapitel wird ersichtlich, dass eine rein deontologische bzw. utilitaristische Ethik voraussichtlich von vielen Menschen abgelehnt w\"urde. Unter keinen Umst\"anden Regeln wie die StVO zu brechen entspricht nicht der heutigen Situation im Stra\ss enverkehr und Berechnungen anzustellen, welche Person zum Wohle einer Mehrheit sterben m\"oge, scheint unvereinbar zu sein mit dem Recht auf Leben und k\"orperliche Unversehrtheit. Einen hybriden Ansatz konstruieren Gerdes und Thornton \autocite{Gerdes2015}, bei dem mithilfe von Kostenfunktionen Entscheidungen getroffen werden sollen:
	\begin{quote}
		\glqq Very direct analogies can be drawn between the frameworks of consequentialism and deontological ethics in philosophy and the use of cost functions or constraints in optimal control theory. These analogies enable ethical principles that can be described as a cost or a rule to be implemented in a control algorithm alongside other objectives. The challenge then becomes determining which principles are best described as a comparative weighting of costs from a consequentialist perspective and which form the more absolute rules of deontological ethics\grqq{} (\autocite{Gerdes2015}, S. 88).
	\end{quote}
	Erneut ist erkennbar, dass die Kostenberechnung ein moralisches Problem darstellt. Es stellt sich wiederholt die Frage, ob ein \emph{allgemeiner} Schaden oder der Schaden der \emph{Insassen} des autonomen Agenten minimiert werden soll. Letzteres k\"onnte zu Situationen f\"uhren, in denen autonome Agenten z.B. bevorzugt Fu\ss g\"anger in Dilemma-Situationen als Kollisionsziel w\"ahlen (vgl. \autocite{Gerdes2015}, S. 92).
	
	Man kann auch keine ethik nehmen und einfach versuchen, Dilemma nicht entstehen zu lassen. 
	
	In Anbetracht der erwarteten Vorz\"uge des autonomen Fahrens mag es ein pragmatischer Ansatz sein, Dilemma-Situationen als seltene Ausnahmef\"alle zu akzeptieren. Laut Gasser \autocite{Gasser2015} sind solche zugespitzten Szenarien kritisch zu bewerten, da der allgemeinen Stra\ss enverkehr zu dynamisch sei, als dass es zwangsweise zu einem t\"odlichen Personenschaden kommen muss. Vielmehr k\"onnten autonome Agenten durch eine defensive Fahrweise solche Situationen rechtzeitig erkennen und, sofern m\"oglich, vermeiden. Sollten Dilemma-Situationen entgegen seiner Meinung von gr\"o\ss erer Relevanz sein, m\"usse man ethische Entscheidungskriterien finden, die allgemein anerkannt seien. (vgl. \autocite{Gasser2015}, S. 555 ff.). Folgendes sollte jedoch nicht vergessen werden: \glqq Auch kommt es in \glq Dilemma-Situationen\grq{} unterschiedslos im Fall eines menschlichen Fahrers wie im Fall maschinellen Wirkens letztlich zu einer Schädigung.\grqq{} (\autocite{Gasser2015}, S. 558).
	
	Bezogen auf das letzte Zitat w\"are es praktikabel, den Insassen eines autonomen Agenten schlicht als \glqq Notfall-Fahrer\grqq{} f\"ur Dilemma-Situationen die Verantwortung zu \"ubergeben. Dieses Vorgehen birgt die meisten Vorteile f\"ur die Hersteller von autonomen Agenten, da sie f\"ur Handlungen des menschlichen Fahrers nicht haftbar gemacht werden k\"onnen. Dieser Ansatz wirkt leidlich praktikabel: Der menschliche Fahrer wird den drohenden Unfall nicht verhindern k\"onnen, da die \"Ubergabe des Fahrauftrags Zeit beansprucht und der Unfall sich in dieser Zeit bereits ereignet hat. Fordert man vom Fahrer wiederum, stets konzentriert den Verkehr zu verfolgen, f\"uhrt dies den Zweck eines autonomen Agenten ad absurdum. Wolf \autocite{Wolf2015} formuliert wie folgt: \glqq einerseits werden Systemfunktionen aufgrund der Fehlerhaftigkeit des Menschen automatisiert, und anderseits soll genau dieser Mensch das System überwachen und im Notfall als Rückfall option zur Verfügung stehen\grqq{} (\autocite{Wolf2015}, S. 105).  Nach Hevelke und Nida-R{\"u}melin \autocite{Hevelke2015} k\"onnte sich mit autonomen Agenten im Durchschnitt zudem nur ca. alle 2 Millionen Kilometer ein Unfall ereignen (\autocite{Hevelke2015}, S. 624 f.). 
	
	Abschlie\ss end sei noch der L\"osungsvorschlag von Scholz und Kempf \autocite{Scholz2016} vorgestellt. Da sowohl der Utilitarismus als auch die Deontologie keine zufriedenstellende Optionen darstellen und zugespitzte Dilemma-Situationen in der Diskussion nicht weiterhelfen, sollen Dilemma durch vier L\"osungswege vermieden werden: eine Graduelle Einf\"uhrung autonomer Agenten in bestimmten Szenarien, Koordination (zwischen autonomen Agenten, anderen Verkehrsteilnehmern und der Infrastruktur), eine defensive Fahrweise und ein nicht-deterministisches Entscheidungssystem. Die Ans\"atze zielen darauf ab, die gesellschaftliche Akzeptanz f\"ur autonome Agenten nach ihrer Einf\"uhrung zu sichern, da sie zun\"achst nur in bestimmten, tendenziell vorhersehbaren Szenarien (z.B. Autobahn) eingesetzt w\"urden und sich etablieren k\"onnen. Eine vernetzte Infrastruktur in Kombination mit einer zur\"uckhaltenden Fahrweise kann im Laufe der fortschreitenden Verbreitung von autonomen Agenten auch in urbanen Gebieten drohende Konflikte erkennen und entsprechend handeln (vgl. \autocite{Scholz2016}, S. 223 ff.). 
	Aus ethischer Sicht ist der Vorschlag, ein nicht-deterministisches Entscheidungssystem zu implementieren, ambivalent. Verhielte sich der autonome Agent \glqq wie ein perfekter Autofahrer\grqq{} (\autocite{Scholz2016}, S. 226), w\"are die Diskussion \"uber ethische Agenten unbegr\"undet, da an menschliche Fahrer keine besondere ethischen Forderungen gestellt worden sind. Andererseits k\"onnten Todesf\"alle im Stra\ss enverkehr auftreten, die mit einem utilitaristischen autonomen Agenten h\"atten verhindert werden k\"onnen. 
\section{Fazit}
	Es konnte kein Konsens \"uber die \glqq richtige\grqq{} Ethik gefunden werden. Generell scheint der utilitaristische Ansatz eher Zustimmung zu finden als andere, jedoch wird Belanglosigkeit des Individuums scharf kritisiert. Speziell in Deutschland wegen seiner nationalsozialistischen Vergangenheit w\"are ein utilitaristischer Ansatz nur schwer vorstellbar (vgl. \autocite{Gless2016}, S. 575). Aus dieser Perspektive betrachtet w\"are im Szenario aus der Einleitung die beste Entscheidung gewesen, emph{nicht} zu entscheiden und mit beiden Frauen zu kollidieren, was jedoch grausamer erscheint als nur ein Todesopfer zu verantworten (vgl. \autocite{Lin2015}, S. 71). Das Dilemma kann mit technischem Fortschritt umgangen, aber im Eintrittfall nicht mehr gel\"ost werden. Es liegt in der Natur eines Dilemmas, dass es keine zufriedenstellende L\"osung geben kann, egal ob Mensch oder autonomer Agent mit einem Dilemma konfrontiert wird. 
	
	Trotzdem besteht Einigkeit dar\"uber, dass die Einf\"uhrung autonomer Agenten nicht wegen womöglich seltener Extremf\"alle verhindert werden sollte. Eine allgemeine gesellschaftliche Diskussion \"uber Ethik in Bezug auf autonome Agenten kann helfen, \emph{realistische} Erwartungen an diese Technologie zu stellen (vgl. \autocite{Lin2015}, S. 82). Transparenz \"uber die Mechanismen von autonomen Agenten hilft (neben bereits genannten negativen Effekten), den Entscheidungsvorgang eines autonomen Agenten zu verstehen und nicht als \glqq Black-Box\grqq{} aufzufassen. 
	
	"Von den Programmierern dieser Fahrzeuge sollte daher nicht das Auflösen moralischer Dilemmata erwartet werden, wenn selbst in der Moralphilosophie keine Einigkeit herrscht" (\autocite{Scholz2016}, S. 222).
 
%Leitfragen nochmal aufgreifen (sind welche offen geblieben?),

\newpage
\printbibliography
\end{document}